{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ©2023 EDF\n",
    "Adrien PETRALIA - EDF R&D and Université Paris Cité (LIPADE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADF & TransApp - Notebook example\n",
    "## A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "root = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(root))\n",
    "from experiments.data_utils import *\n",
    "from src.TransAppModel.TransApp import *\n",
    "from src.AD_Framework.Framework import *\n",
    "from src.utils.losses import *\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of a TransApp Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inst(m, win, dim_model, mode=\"pretraining\", large_version=False, path_select_core=None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Get TransApp model instance\n",
    "    \n",
    "    Parameters:\n",
    "        m: int - n channel of input time series\n",
    "        win: int - length of input subsequence (usefull for positional encoding, if any)\n",
    "        mode: str - 'pretraining' or 'classif' (type of head)\n",
    "        large_version: boolean - if true, use 5 encoder layers instead of 3\n",
    "        path_select_core: str - path to pretrained instance of TransApp \n",
    "    \"\"\"\n",
    "\n",
    "    TApp = TransApp(max_len=win, c_in=m,\n",
    "                    mode=mode,\n",
    "                    n_embed_blocks=1, \n",
    "                    encoding_type='noencoding',\n",
    "                    n_encoder_layers=5 if large_version else 3,\n",
    "                    kernel_size=5,\n",
    "                    d_model=dim_model, pffn_ratio=2, n_head=4,\n",
    "                    prenorm=True, norm=\"LayerNorm\",\n",
    "                    activation='gelu',\n",
    "                    store_att=False, attn_dp_rate=0.2, head_dp_rate=0., dp_rate=0.2,\n",
    "                    att_param={'attenc_mask_diag': True, 'attenc_mask_flag': False, 'learnable_scale_enc': False},\n",
    "                    c_reconstruct=1, apply_gap=True, nb_class=2)\n",
    "\n",
    "    if path_select_core is not None:\n",
    "        TApp.load_state_dict(torch.load(path_select_core)['model_state_dict'])\n",
    "\n",
    "    return TApp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-supervised pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "TransApp                                                [1, 1, 1024]              194\n",
       "├─Sequential: 1-1                                       [1, 1024, 96]             --\n",
       "│    └─DilatedBlock: 2-1                                [1, 96, 1024]             --\n",
       "│    │    └─Sequential: 3-1                             [1, 96, 1024]             139,872\n",
       "│    └─Transpose: 2-2                                   [1, 1024, 96]             --\n",
       "├─Sequential: 1-2                                       [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-3                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-2                              [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-3                         [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-4                              [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-5                [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-6                                [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-4                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-7                              [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-8                         [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-9                              [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-10               [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-11                               [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-5                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-12                             [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-13                        [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-14                             [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-15               [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-16                               [1, 1024, 96]             --\n",
       "│    └─LayerNorm: 2-6                                   [1, 1024, 96]             192\n",
       "├─Sequential: 1-3                                       [1, 1024, 1]              --\n",
       "│    └─Linear: 2-7                                      [1, 1024, 1]              97\n",
       "│    └─Dropout: 2-8                                     [1, 1024, 1]              --\n",
       "=========================================================================================================\n",
       "Total params: 364,707\n",
       "Trainable params: 364,707\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 142.67\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 28.32\n",
       "Params size (MB): 1.46\n",
       "Estimated Total Size (MB): 29.78\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m       = 1    # Number of channel of the input time series (i.e. consumption time series, hours encoded in sin/cos based , days encoded in sin/cos based)\n",
    "win     = 1024 # Choseen length of slicing window size\n",
    "d_model = 96   # Inner dimension of the model\n",
    "\n",
    "TransAppInstance = get_model_inst(m=m, win=win, dim_model=d_model, mode=\"pretraining\") # Pretraining mode of our TransApp model\n",
    "\n",
    "summary(TransAppInstance, input_size=(1, m, win), mode=\"train\", device='cuda') # show TransApp architecture with pretraining head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ComStock 15min data for pretraining: (1000, 35040)\n",
      "Applied entire curve normalization\n",
      "Pretraining data shape: (34000, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# data_pretraining = CER_get_data_pretraining(exo_variable=['hours_cos', 'hours_sin', 'days_cos', 'days_sin'])\n",
    "data_pretraining = COMSTOCK_get_data_pretraining(resolution='15min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrainer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'lr': 1e-4, 'wd': 1e-4, 'batch_size': 16, 'epochs': 1}\n",
    "\n",
    "#create the tmp folder if it does not exist\n",
    "\n",
    "save_path = str(root) + '/tmp/TransAppPT' # Model save path\n",
    "# Create tmp directory if it doesn't exist\n",
    "tmp_dir = Path(root) / 'tmp'\n",
    "tmp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "pretraining_dataset = TSDataset(data_pretraining, scaler=True, scale_dim=[0])\n",
    "train_loader = torch.utils.data.DataLoader(pretraining_dataset, batch_size=dict_params['batch_size'], shuffle=True)\n",
    "\n",
    "GeomMask = GeometricMask(mean_length=24, masking_ratio=0.5, type_corrupt='zero', dim_masked=0) # Mask to corrupt inout time series\n",
    "\n",
    "model_pretrainer = self_pretrainer(TransAppInstance,                                     \n",
    "                                   train_loader, valid_loader=None,\n",
    "                                   learning_rate=dict_params['lr'], weight_decay=dict_params['wd'],\n",
    "                                   name_scheduler='CosineAnnealingLR',\n",
    "                                   dict_params_scheduler={'T_max': dict_params['epochs'], 'eta_min': 1e-6},\n",
    "                                   warmup_duration=None,\n",
    "                                   criterion=MaskedMSELoss(type_loss='L1'), mask=GeomMask,\n",
    "                                   device=\"cuda\", all_gpu=False,\n",
    "                                   verbose=True, plotloss=True, \n",
    "                                   save_fig=False, path_fig=None,\n",
    "                                   save_only_core=False,\n",
    "                                   save_checkpoint=True, path_checkpoint=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_pretrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/AD_Framework/Framework.py:223\u001b[39m, in \u001b[36mself_pretrainer.train\u001b[39m\u001b[34m(self, n_epochs)\u001b[39m\n\u001b[32m    220\u001b[39m t = time.time()\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# =======================one epoch===================== #\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss_train_history.append(train_loss)\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.valid_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/AD_Framework/Framework.py:319\u001b[39m, in \u001b[36mself_pretrainer.__train\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_masked\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     loss    = \u001b[38;5;28mself\u001b[39m.criterion(outputs, ts.to(\u001b[38;5;28mself\u001b[39m.device), mask_loss.to(\u001b[38;5;28mself\u001b[39m.device))\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/TransAppModel/TransApp.py:350\u001b[39m, in \u001b[36mTransApp.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    347\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.PosEncoding(x)\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# Forward Encoder\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mEncoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# Forward Head\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode==\u001b[33m\"\u001b[39m\u001b[33mpretraining\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/TransAppModel/TransApp.py:231\u001b[39m, in \u001b[36mEncoderLayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prenorm:\n\u001b[32m    230\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m new_x, att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.store_att:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.att = att\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/TransAppModel/TransApp.py:173\u001b[39m, in \u001b[36mAttentionLayer.forward\u001b[39m\u001b[34m(self, queries, keys, values)\u001b[39m\n\u001b[32m    170\u001b[39m keys    = \u001b[38;5;28mself\u001b[39m.key_projection(keys).view(B, S, H, -\u001b[32m1\u001b[39m)\n\u001b[32m    171\u001b[39m values  = \u001b[38;5;28mself\u001b[39m.value_projection(values).view(B, S, H, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m out, att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m out = out.view(B, L, -\u001b[32m1\u001b[39m)\n\u001b[32m    175\u001b[39m out = \u001b[38;5;28mself\u001b[39m.Dropout(\u001b[38;5;28mself\u001b[39m.out_projection(out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/transapp-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ISP/TransApp/src/TransAppModel/TransApp.py:133\u001b[39m, in \u001b[36mScaleDotProductAttention.forward\u001b[39m\u001b[34m(self, queries, keys, values, attn_mask)\u001b[39m\n\u001b[32m    130\u001b[39m     diag_mask = DiagonalMask(B, L, device=queries.device)\n\u001b[32m    131\u001b[39m     scores.masked_fill_(diag_mask.mask, -np.inf)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m A = \u001b[38;5;28mself\u001b[39m.dropout(torch.softmax(\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m, dim=-\u001b[32m1\u001b[39m))\n\u001b[32m    134\u001b[39m V = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mbhls,bshd->blhd\u001b[39m\u001b[33m\"\u001b[39m, A, values)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_attention:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "model_pretrainer.train(dict_params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the pretrained model for Appliance Detection (i.e., a chosen classification case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "TransApp                                                [1, 2]                    97\n",
       "├─Sequential: 1-1                                       [1, 1024, 96]             --\n",
       "│    └─DilatedBlock: 2-1                                [1, 96, 1024]             --\n",
       "│    │    └─Sequential: 3-1                             [1, 96, 1024]             142,368\n",
       "│    └─Transpose: 2-2                                   [1, 1024, 96]             --\n",
       "├─Sequential: 1-2                                       [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-3                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-2                              [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-3                         [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-4                              [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-5                [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-6                                [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-4                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-7                              [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-8                         [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-9                              [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-10               [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-11                               [1, 1024, 96]             --\n",
       "│    └─EncoderLayer: 2-5                                [1, 1024, 96]             --\n",
       "│    │    └─LayerNorm: 3-12                             [1, 1024, 96]             192\n",
       "│    │    └─AttentionLayer: 3-13                        [1, 1024, 96]             37,248\n",
       "│    │    └─LayerNorm: 3-14                             [1, 1024, 96]             192\n",
       "│    │    └─PositionWiseFeedForward: 3-15               [1, 1024, 96]             37,152\n",
       "│    │    └─Dropout: 3-16                               [1, 1024, 96]             --\n",
       "│    └─LayerNorm: 2-6                                   [1, 1024, 96]             192\n",
       "├─Sequential: 1-3                                       [1, 2]                    --\n",
       "│    └─Transpose: 2-7                                   [1, 96, 1024]             --\n",
       "│    └─AdaptiveAvgPool1d: 2-8                           [1, 96, 1]                --\n",
       "│    └─Flatten: 2-9                                     [1, 96]                   --\n",
       "│    └─Linear: 2-10                                     [1, 2]                    194\n",
       "│    └─Dropout: 2-11                                    [1, 2]                    --\n",
       "=========================================================================================================\n",
       "Total params: 367,203\n",
       "Trainable params: 367,203\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 145.22\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 29.10\n",
       "Params size (MB): 1.47\n",
       "Estimated Total Size (MB): 30.59\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransAppInstance.mode = \"classif\" # Change the mode of the TransApp architecture, i.e. use a classification head\n",
    "# OR\n",
    "#TransAppInstance = get_model_inst(m=m, win=win, dim_model=d_model, mode=\"classif\", path_select_core= str(root) + '/tmp/TransAppPT.pt') # Load previous pretrained instance\n",
    "\n",
    "summary(TransAppInstance, input_size=(1, m, win), mode=\"train\", device='cpu') # show TransApp architecture with classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a possible detection case on CER dataset\n",
    "\n",
    "- cooker_case\n",
    "- dishwasher_case\n",
    "- waterheater_case\n",
    "- pluginheater_case\n",
    "- tumbledryer_case\n",
    "- tv_greater21inch_case\n",
    "- tv_lessr21inch_case\n",
    "- desktopcomputer_case\n",
    "- laptopcomputer_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'cooker_case' # exemple of detecting cooker in consumption series\n",
    "\n",
    "datas_tuple = CER_get_data_case('cooker_case', seed=0, exo_variable=['hours_cos', 'hours_sin', 'days_cos', 'days_sin'], win=win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD Framework instance and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'lr': 1e-4, 'wd': 1e-3, 'batch_size': 16, 'epochs': 10, 'p_es': 5, 'p_rlr': 3, 'n_warmup_epochs': 1}\n",
    "save_path = str(root) + '/tmp/TransAppPTFinetuned'\n",
    "\n",
    "# Scliced data for training\n",
    "X_train = datas_tuple[0]\n",
    "y_train = datas_tuple[1]\n",
    "X_valid = datas_tuple[2]\n",
    "y_valid = datas_tuple[3]\n",
    "X_test  = datas_tuple[4]\n",
    "y_test  = datas_tuple[5]\n",
    "\n",
    "# Entire curves data for evaluate the model\n",
    "X_train_voter = datas_tuple[6]\n",
    "y_train_voter = datas_tuple[7]\n",
    "X_valid_voter = datas_tuple[8]\n",
    "y_valid_voter = datas_tuple[9]\n",
    "X_test_voter  = datas_tuple[10]\n",
    "y_test_voter  = datas_tuple[11]\n",
    "\n",
    "# Dataset\n",
    "train_dataset = TSDataset(X_train, y_train, scaler=True, scale_dim=[0])\n",
    "valid_dataset = TSDataset(X_valid, y_valid, scaler=True, scale_dim=[0])\n",
    "test_dataset  = TSDataset(X_test,  y_test,  scaler=True, scale_dim=[0])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=dict_params['batch_size'], shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# AD Framework trainer\n",
    "model_trainer = AD_Framework(TransAppInstance,\n",
    "                             train_loader=train_loader, valid_loader=valid_loader,\n",
    "                             learning_rate=dict_params['lr'], weight_decay=dict_params['wd'],\n",
    "                             criterion=nn.CrossEntropyLoss(),\n",
    "                             patience_es=dict_params['p_es'], patience_rlr=dict_params['p_rlr'],\n",
    "                             f_metrics=getmetrics(),\n",
    "                             n_warmup_epochs=dict_params['n_warmup_epochs'],\n",
    "                             scale_by_subseq_in_voter=True, scale_dim=[0],\n",
    "                             verbose=True, plotloss=True, \n",
    "                             save_fig=False, path_fig=None,\n",
    "                             device=\"cuda\", all_gpu=False,\n",
    "                             save_checkpoint=True, path_checkpoint=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "    Train loss : 0.6287, Train acc : 65.02%\n",
      "    Valid  loss : 0.5774, Valid  acc : 74.43%\n",
      "Epoch [2/10]\n",
      "    Train loss : 0.5904, Train acc : 69.06%\n",
      "    Valid  loss : 0.5581, Valid  acc : 74.57%\n",
      "Epoch [3/10]\n",
      "    Train loss : 0.5715, Train acc : 70.93%\n",
      "    Valid  loss : 0.5686, Valid  acc : 72.63%\n",
      "Epoch [4/10]\n",
      "    Train loss : 0.5555, Train acc : 72.17%\n",
      "    Valid  loss : 0.5570, Valid  acc : 74.06%\n",
      "Epoch [5/10]\n",
      "    Train loss : 0.5396, Train acc : 73.28%\n",
      "    Valid  loss : 0.5335, Valid  acc : 75.21%\n",
      "Epoch [6/10]\n",
      "    Train loss : 0.5249, Train acc : 74.24%\n",
      "    Valid  loss : 0.6050, Valid  acc : 70.70%\n",
      "Epoch [7/10]\n",
      "    Train loss : 0.5063, Train acc : 75.40%\n",
      "    Valid  loss : 0.5839, Valid  acc : 74.42%\n",
      "Epoch [8/10]\n",
      "    Train loss : 0.4931, Train acc : 76.50%\n",
      "    Valid  loss : 0.5867, Valid  acc : 74.92%\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [9/10]\n",
      "    Train loss : 0.4742, Train acc : 77.63%\n",
      "    Valid  loss : 0.6597, Valid  acc : 70.38%\n",
      "Early stopping after 10 epochs !\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_trainer.train(dict_params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored best model met during training.\n",
      "{'ACCURACY': 0.7403156384505022, 'PRECISION': 0.4696969696969697, 'RECALL': 0.5502958579881657, 'PRECISION_MACRO': 0.6586961802392663, 'RECALL_MACRO': 0.6757161108122647, 'F1_SCORE': 0.5068119891008176, 'F1_SCORE_MACRO': 0.6652852545309347, 'F1_SCORE_WEIGHTED': 0.7469092176434339, 'CONFUSION_MATRIX': array([[ 93,  76],\n",
      "       [105, 423]]), 'ROC_AUC_SCORE': 0.7523085888470503, 'ROC_AUC_SCORE_MACRO': 0.7523085888470503, 'ROC_AUC_SCORE_WEIGHTED': 0.7523085888470503}\n"
     ]
    }
   ],
   "source": [
    "#============ eval last model on subsequences ============#\n",
    "model_trainer.evaluate(torch.utils.data.DataLoader(test_dataset, batch_size=1), mask='test_metrics_lastmodel')\n",
    "\n",
    "#============ restore best weight ============#    \n",
    "model_trainer.restore_best_weights()\n",
    "\n",
    "#============ eval model on subsequences  ============#   \n",
    "model_trainer.evaluate(torch.utils.data.DataLoader(test_dataset, batch_size=1))\n",
    "\n",
    "#============ find best quantile on valid voter dataset ============#\n",
    "model_trainer.ADFFindBestQuantile(TSDataset(X_valid_voter, y_valid_voter), m=m, win=win)\n",
    "\n",
    "#============ evaluate on test voter dataset using best quantile ============#\n",
    "quant_metric = model_trainer.ADFvoter_proba(TSDataset(X_test_voter, y_test_voter), m=m, win=win)\n",
    "print(quant_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transapp-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
